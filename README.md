# simple-VLA
Reproducible and easy-to-use Vision-Language-Action (VLA) baselines.

## TODO

- [ ] **Task & Scene Description**: Design and define tasks using a manipulator in Mujoco. Tasks are yet to be determined, but may include `pick-n-place, push, insert, hang, etc`. Considering both single-arm and bimanipulation scenarios.

- [ ] **Collect Demonstration Pipeline**: A pipeline to collect demonstrations for the defined tasks.

- [ ] **Training**: Implement training procedures for VLA models using the collected demonstrations.

- [ ] **Deploy**: Deploy the trained models for evaluation and real-world testing.

---

## Acknowledgement

Most of the code is based on [yet-another-mujoco-tutorial-v3](https://github.com/sjchoi86/yet-another-mujoco-tutorial-v3) by [sjchoi86](https://github.com/sjchoi86).

